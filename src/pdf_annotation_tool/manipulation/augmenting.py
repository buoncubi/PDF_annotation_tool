# -*- coding: utf-8 -*-
# -----------------------------------------------------------------------------
# Author: Luca Buoncompagni
# Version: 1.0
# Date: December 2025
# License: GNU Affero General Public License v3.0 (AGPL-3.0)
#
# This program is free software: you can redistribute it and/or modify
# it under the terms of the GNU Affero General Public License as published
# by the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with this program. If not, see <https://www.gnu.org/licenses/>.
# -----------------------------------------------------------------------------

import re
import time
import traceback
import yaml

from dataclasses import dataclass
from typing import List, Tuple, Dict, TYPE_CHECKING
from pathlib import Path

from PyQt5.QtWidgets import QDialog, QVBoxLayout, QGridLayout, QSplitter, QHBoxLayout, QTextEdit, QLabel, QPushButton, QCheckBox, QLineEdit, QMessageBox, QProgressBar, QComboBox, QWidget, QDoubleSpinBox, QSpinBox
from PyQt5.QtCore import Qt, pyqtSignal, QThread

from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage

from pdf_annotation_tool.manipulation.editor import SelectionDataEditingWidget
from pdf_annotation_tool.selection.data import SelectionCategory, SelectionData
from pdf_annotation_tool.selection.manager import SelectionsManager
if TYPE_CHECKING:
    from pdf_annotation_tool.tool import PDFAnnotationTool


@dataclass
class LlmInputs:
    """Configuration data structure for LLM API requests.
    
    Contains all parameters needed to make a request to the OpenAI API including
    prompt, image data, model settings, and selection categories.
    """
    prompt: str     # The prompt to instruct the LLM model
    image: str      # The image to pass to the LLM. No image is passed if it is None
    temp: float     # The temperature config for the LLM model
    top_p: float    # The top P config for the LLM model
    max_token: int  # The maximum number of token the LLM can generate
    model: str      # The OpenAI model name, e.g., "gpt-4o"
    api_key: str    # The OpenAI API key
    sel_categories: set[str] # The selection category to reason with



@dataclass
class LlmOutput:
    """Response data structure from LLM API calls.
    
    Contains the generated response text along with usage statistics
    for monitoring and analytics purposes.
    """

    response: str   # The LLM response to the prompt
    in_token: int   # Counter of input tokens (for analytics)
    out_token: int  # Counter of output tokens (for analytics)
    time: int       # Computation time in milliseconds



# TODO use `ProgressingRunner` also for LLM invocation
class LlmWorker(QThread):
    """Asynchronous worker thread for LLM API calls.
    
    Handles OpenAI API invocation in a separate thread to prevent UI blocking.
    Supports both real API calls and simulation mode for testing tha can be enabled from the `SHOULD_SIMULATE` boolean.
    """
    
    # Signal generated by this class.
    finished = pyqtSignal(object)
    error = pyqtSignal(Exception)

    # Default simulation flag.
    SHOULD_SIMULATE = False


    def __init__(self, llm_inputs: LlmInputs, parent: QWidget = None, should_simulate: bool = SHOULD_SIMULATE):
        """Initialize LLM worker with configuration.
        
        Args:
            llm_inputs (LlmInputs): Configuration for the LLM request.
            parent (QWidget, optional): Parent widget.
            should_simulate (bool): Whether to simulate API calls instead of real ones.
            
        Returns:
            None
        """

        super().__init__(parent)
        self.llm_inputs = llm_inputs

        self.should_simulate = should_simulate
        if self.should_simulate:
            self.simulate_delay_ms = 2000
            
        self._cancelled = False


    def cancel(self) -> None:
        """Cancel the ongoing LLM request."""

        self._cancelled = True


    def run_simulation(self) -> None:
        """Run simulated LLM call for testing purposes. It return a dummy response.
        The results are given as an `LlmOutput` object streamed through a signal within `self.finish`."""
        
        # Simulated wait
        waited = 0
        while waited < self.simulate_delay_ms / 1000.0:
            if self._cancelled:
                return 
            time.sleep(0.1)
            waited += 0.1
        dummy_result = {
            "text": f"(Simulated) Response for prompt: {self.llm_inputs.prompt}",
            "usage": {"prompt_tokens": 0, "completion_tokens": 0, "total_tokens": 0}
        }
        dummy_result = LlmOutput(
            response="A dummy response",
            in_token=-1,
            out_token=-1,
            time=-1
        )
        self.finished.emit(dummy_result)


    def invoke_openai(self) -> None:
        """Invoke the OpenAI API using LangChain with inputs based on `self.llm_inputs` given at construction time.
        The results are given as an `LlmOutput` object streamed through a signal within `self.finish`.
        """
        
        # Prepare the message content
        content = []
        if self.llm_inputs.prompt:
            content.append({"type": "text", "text": self.llm_inputs.prompt})
        if self.llm_inputs.image:
            content.append({
                "type": "image_url",
                "image_url": {"url": f"data:image/png;base64,{self.llm_inputs.image}"}
            })
        message = HumanMessage(content=content)
        
        # Prepare OPENAI
        llm = ChatOpenAI(
            model = self.llm_inputs.model,
            temperature = self.llm_inputs.temp,
            top_p = self.llm_inputs.top_p,
            max_tokens = self.llm_inputs.max_token,
            openai_api_key = self.llm_inputs.api_key
        )
        
        # Save time for statistics and invoke the LLM
        start_time = time.time()
        result = llm.invoke([message])
        elapsed_time = int((time.time() - start_time) * 1000)  # in ms

        # Extract token usage info if available
        in_tokens = result.response_metadata["token_usage"]["prompt_tokens"]
        out_tokens = result.response_metadata["token_usage"]["completion_tokens"]

        # Prepare the output and emit finished signal.
        result = LlmOutput(
            response=result.content if hasattr(result, 'content') else str(result),
            in_token=in_tokens,
            out_token=out_tokens,
            time=elapsed_time
        )
        self.finished.emit(result)
    
    
    def run(self):
        """Main thread execution method. Calls either simulation or real API, i.e., `run_simulation()` or `invoke_openai()`."""

        try:
            if self.should_simulate:
                self.run_simulation()
            else:
                self.invoke_openai()
        except Exception as e:
            self.error.emit(e)
            traceback.print_exc()



class LoadingDialog(QDialog):
    """Modal dialog showing progress during LLM API calls.
    
    Displays a progress bar and allows users to cancel ongoing requests.
    """
    
    # The signal to stop waiting
    stop_requested = pyqtSignal()


    def __init__(self, message: str, parent: QWidget = None):
        """Initialize an indefinite loading dialog with a message and a cancel button.
        
        Args:
            message (str): Message to display to user.
            parent (QWidget, optional): Parent widget.
            
        Returns:
            None
        """
        
        # Init graphic
        super().__init__(parent)
        self.setWindowTitle("Loading...")
        self.setModal(True)
        self.resize(300, 100)

        layout = QVBoxLayout()
        self.label = QLabel(message)
        
        progress_bar = QProgressBar()
        progress_bar.setRange(0, 0)
        layout.addWidget(progress_bar)
        
        stop_button = QPushButton("Stop")

        layout.addWidget(self.label)
        layout.addWidget(stop_button)
        self.setLayout(layout)

        stop_button.clicked.connect(self.on_stop)


    def on_stop(self) -> None:
        """Handle stop button click by emitting `stop_requested` signal."""

        self.label.setText("Stopping...")
        self.stop_requested.emit()



# A custom window that allow setting up the LLM request
class LlmConfigWidget(QWidget):
    """Configuration widget for setting up LLM requests.
    
    Provides UI controls for API key, model selection, prompt configuration,
    category selection, and model parameters like temperature and `top_p`.
    """

    DEFAULT_TEMP = 0.5
    DEFAULT_TOPP = 0.9
    DEFAULT_IMAGE_SIZE = 512

    # Prompt placeholder for selections related to the one currently considered 
    PLACEHOLDER_TITLE_PATH = "title_path"
    PLACEHOLDER_CONTEXT = "context"
    PLACEHOLDER_CONTEXT_SIZE = 5 # TODO set contextualization limit
    SPECIAL_PROMPT_PLACEHOLDER = set([PLACEHOLDER_TITLE_PATH, PLACEHOLDER_CONTEXT])

    PROMPT_INSTRUCTION = f"""Insert prompt from scratch or chose a template.
    You can add section properties with placeholder like
        - `{{{{id_}}}}` -> the selections's identifier.
        - `{{{{doc}}}}` -> the file path or url to the PDF document
        - `{{{{page}}}}` -> the PDF page.
        - `{{{{idx}}}}` -> the index of this selection within the page.
        - `{{{{coords}}}}` -> The selection's polygon vertexes in the PDF coordinates.
        - `{{{{text}}}}` -> The text extracted within the selection coordinates.
        - `{{{{category}}}}` -> The type of the selection
        - `{{{{image}}}}` -> The screenshot within the selection coordinates. (USE THE DEDICATED CHECKBOX INSTEAD)
        - `{{{{parent}}}}` -> The parent selection ID (`None` if it is the root)
        - `{{{{children}}}}` -> The children selection ID (`[]` if it is a leaf)
        - `{{{{description}}}}` -> The optional LLM-based description of this selection
        - `{{{{title_path}}}}` -> The list of parents's `text` including itself.
        - `{{{{context}}}}` -> The `text` of the N-th closest siblings nodes, where N is specified by `Discovering Context Size` below.
        
    Example:
    \"Describe the image {{{{id_}}}}, which contains the text: {{{{text}}}}\".
    """

    # A JSON file path where precompiled prompts are stored
    PROMPTS_PATH = "resources/default_prompts.yaml"
    
    
    def __init__(self, llm_inputs: LlmInputs = None, parent: QWidget = None, allow_api_config: bool = True):
        """Initialize LLM configuration widget.
        
        Args:
            llm_inputs (LlmInputs, optional): Initial configuration values.
            parent (QWidget, optional): Parent widget.
            
        Returns:
            None
        """

        super().__init__(parent)

        self.setWindowTitle("LLM Config")
        layout = QVBoxLayout()
        
        #self._allow_api_config = allow_api_config
        self.api_edit = QLineEdit()
        self.api_key_label = QLabel("API Key:")
        layout.addWidget(self.api_key_label)
        layout.addWidget(self.api_edit)
        
        self.api_model_edit = QLineEdit()
        self.api_model_label = QLabel("API Model:")
        layout.addWidget(self.api_model_label)
        layout.addWidget(self.api_model_edit)

        self.category_label = QLabel("Select Categories:")
        layout.addWidget(self.category_label)
        self.category_checks = {}
    
        category_layout = QGridLayout()
        category_layout.setContentsMargins(0, 0, 0, 0)
        category_layout.setSpacing(2)
        for i, category in enumerate(SelectionCategory):
            row, col = divmod(i, 6)
            category_name = category.value.name
            cb = QCheckBox(category_name)
            self.category_checks[category_name] = cb
            #cb.setSizePolicy(QSizePolicy.Fixed, QSizePolicy.Fixed)
            category_layout.addWidget(cb, row, col)
        layout.addLayout(category_layout)        
    
        self.prompt_dropdown = QComboBox()
        prompts_name = ["Select a default prompt"]
        self.prompt_map = self.load_prompt_map()
        prompts_name.extend(list(self.prompt_map.keys()))   
        self.prompt_dropdown.addItems(prompts_name)
        first_item = self.prompt_dropdown.model().item(0)
        first_item.setFlags(first_item.flags() & ~Qt.ItemIsEnabled) # Set the first item unselectable
        
        layout.addWidget(QLabel("Prompt:"))
        layout.addWidget(self.prompt_dropdown)
        self.prompt_dropdown.currentTextChanged.connect(self.on_default_prompt_selected)
        self.prompt_edit = QTextEdit()
        layout.addWidget(self.prompt_edit)

        param_layout = QHBoxLayout()
        self.temp_spin = QDoubleSpinBox()
        self.temp_spin.setRange(0,1)
        self.temp_spin.setSingleStep(0.1)  
        self.temp_spin.setDecimals(2)
        
        self.top_p_spin = QDoubleSpinBox()
        self.top_p_spin.setRange(0,1)
        self.top_p_spin.setSingleStep(0.1)  
        self.top_p_spin.setDecimals(2)
        
        self.max_tokens_spin = QSpinBox()
        self.max_tokens_spin.setRange(1,5000)
        
        separator = 15
        param_layout.addWidget(QLabel("Temp:")) 
        param_layout.addWidget(self.temp_spin)
        param_layout.addSpacing(separator)
        param_layout.addWidget(QLabel("Top P:"))
        param_layout.addWidget(self.top_p_spin)
        param_layout.addSpacing(separator)
        param_layout.addWidget(QLabel("Max Tokens:"))
        param_layout.addWidget(self.max_tokens_spin)
        param_layout.addSpacing(separator)
        layout.addLayout(param_layout)
        
        hor_layout = QHBoxLayout()
        self.include_img_cb = QCheckBox("Include image")
        hor_layout.addWidget(self.include_img_cb)
        hor_layout.addSpacing(separator)
        discover_label =QLabel("Discovering Context Size:")
        hor_layout.addWidget(discover_label)
        self.discover_spin = QDoubleSpinBox()
        self.discover_spin.setRange(1,10000)
        self.discover_spin.setSingleStep(1)  
        self.discover_spin.setDecimals(0)
        self.discover_spin.setValue(LlmConfigWidget.PLACEHOLDER_CONTEXT_SIZE)
        discover_tip = "Set the number of contextualising selections retrieved by the prompt placeholder: `{{context}}`. "
        self.discover_spin.setToolTip(discover_tip)
        discover_label.setToolTip(discover_tip)
        hor_layout.addWidget(self.discover_spin)
        layout.addLayout(hor_layout)
        
        self.setLayout(layout)
        
        self._set_llm_inputs(llm_inputs, allow_api_config)
      

    def load_prompt_map(self, PROMPTS_PATH: str = PROMPTS_PATH) -> Dict[str, str]:
        """
        Load prompt templates from a YAML file.

        The YAML file must define a mapping from prompt names (strings)
        to prompt templates (multiline strings). Multiline prompts are
        expected to be written using YAML block scalars (|), allowing
        real newlines, Markdown, and code blocks without escaping.

        Example YAML structure:

            Prompt for Images: |
                Multiline prompt text...
            Prompt for Tables: |
                Another multiline prompt...

        If the file does not exist or cannot be parsed, a Qt warning dialog
        is shown using QMessageBox.warning, and an empty dictionary is returned.

        Parameters
        ----------
        self :
            Qt widget or main window instance used as parent for QMessageBox.
        PROMPTS_PATH : str
            Path to the YAML file containing the prompt definitions.

        Returns
        -------
        Dict[str, str]
            A dictionary mapping prompt names to prompt templates.
            Returns an empty dictionary on error.
        """
        path = Path(PROMPTS_PATH)

        if not path.exists():
            QMessageBox.warning(
                self,
                "Warning",
                f"Prompt file not found:\n\n{path}"
            )
            return {}

        try:
            with path.open("r", encoding="utf-8") as f:
                data = yaml.safe_load(f)

            if not isinstance(data, dict):
                raise ValueError("The YAML root element must be a mapping.")

            # Ensure keys and values are strings
            return {str(k): str(v) for k, v in data.items()}

        except Exception as exc:
            QMessageBox.warning(
                self,
                "Warning",
                f"Failed to load prompt file:\n\n{exc}"
            )
            return {}






        
    def _set_llm_inputs(self, llm_inputs: LlmInputs, allow_api_config:bool) -> None:
        """Update widget controls with given LLM configuration.
        
        Args:
            llm_inputs (LlmInputs): Configuration to load into the widget.
            
        Returns:
            None
        """
        
        def reset_category_checkboxes(cb_dict):
            for cb in cb_dict.values():
                cb.setChecked(False)
        
        if llm_inputs is None: # Initialize with default values
            self.api_edit.setPlaceholderText("Insert OPENAI API Key")
            self.api_model_edit.setPlaceholderText("Insert the OPENAI model name, e.g., `gpt-4.1-mini`")
            self.prompt_edit.setPlaceholderText(LlmConfigWidget.PROMPT_INSTRUCTION)
            self.prompt_edit.setToolTip(LlmConfigWidget.PROMPT_INSTRUCTION)
            reset_category_checkboxes(self.category_checks)
            self.temp_spin.setValue(LlmConfigWidget.DEFAULT_TEMP)
            self.top_p_spin.setValue(LlmConfigWidget.DEFAULT_TOPP)
            self.max_tokens_spin.setValue(LlmConfigWidget.DEFAULT_IMAGE_SIZE)
            self.include_img_cb.setChecked(False)
        else: # Maintain the last user settings
            self.api_edit.setText(llm_inputs.api_key)
            self.api_model_edit.setText(llm_inputs.model)
            self.prompt_edit.setText(llm_inputs.prompt)
            reset_category_checkboxes(self.category_checks)
            for t in llm_inputs.sel_categories:
                self.category_checks[t].setChecked(True)
            self.temp_spin.setValue(llm_inputs.temp)
            self.top_p_spin.setValue(llm_inputs.top_p)
            self.max_tokens_spin.setValue(llm_inputs.max_token)
            self.include_img_cb.setChecked(llm_inputs.image is not None)
            
        if not allow_api_config:
            self.api_edit.setHidden(True)
            self.api_model_edit.setHidden(True)
            self.prompt_dropdown.setHidden(True)
            self.api_key_label.setHidden(True)
            self.api_model_label.setHidden(True)
            self.category_label.setHidden(True)
            for _, checkbox in self.category_checks.items():
                checkbox.setHidden(True)


    def get_llm_inputs(self, selection_data: SelectionData, manager: SelectionsManager) -> LlmInputs:
        """Extract current widget values into `LlmInputs` configuration.
        
        Validates required fields (API key, model, prompt) and selected categories.
        Optionally processes prompt placeholders and includes image data from selection.
        
        Args:
            selection_data (SelectionData, optional): Selection data for prompt processing and image inclusion.
            
        Returns:
            LlmInputs: Current configuration from widget controls, or None if validation fails.
        """

        # Check required user input: API KEY
        api_key = self.api_edit.text().strip()
        if not api_key:
            QMessageBox.warning(self, "Error","API key required!")
            return None
        
        # Check required user input: API MODEL
        api_model = self.api_model_edit.text().strip()
        if not api_model:
            QMessageBox.warning(self,"Error","API model required!")
            return None

        # Check required user input: PROMPT
        prompt = self.prompt_edit.toPlainText().strip()
        if prompt is None or prompt == "":
            QMessageBox.warning(self,"Error","Prompt required!")
            return None
        
        selected_categories = self.get_selected_categories()
        if selected_categories is None: # It already shows an alert to the user.
            return None
        
        # Populate prompt inputs with data extracted from the PDF selection
        image = None
        if selection_data is not None:
            discover_size = int(self.discover_spin.value())
            prompt = self.parse_prompt(prompt, selection_data, manager, discover_size) # It shows an alert in case of error
            if prompt is None:
                return None
            if self.include_img_cb.isChecked():
                image = selection_data.image
                
        return LlmInputs(
            prompt       = prompt,
            image        = image,
            temp         = self.temp_spin.value(),
            top_p        = self.top_p_spin.value(),
            max_token    = self.max_tokens_spin.value(),
            model        = api_model,
            api_key      = api_key,
            sel_categories = selected_categories
        )


    def get_selected_categories(self) -> List[str]:
        """Get list of selected category names from checkboxes.
        
        Validates that at least one category is selected and shows warning if none selected.
        
        Returns:
            str: the selected category names, or None if no categories selected.
        """

        selected_categories = []
        for cb in self.category_checks.values():
            if cb.isChecked():
                selected_categories.append(cb.text()) #= cb.text()
                #break # only one category can be selected at the same time
            
        if not selected_categories:
            QMessageBox.warning(self, "Error", "At least one selection's category is required!")
            return None
        
        return selected_categories


    def on_default_prompt_selected(self, text: str) -> None:
        """Handle selection of default prompt from dropdown.
        
        Sets the prompt text editor with the selected template and resets dropdown
        to default state without triggering recursive selection events.
        
        Args:
            text (str): The selected prompt template name from dropdown.
            
        Returns:
            None
        """
    
        self.prompt_edit.setPlainText(self.prompt_map.get(text, ""))
        # Reset the dropdown menu selection without triggering selection event recursively
        self.prompt_dropdown.blockSignals(True)
        self.prompt_dropdown.setCurrentIndex(0)
        self.prompt_dropdown.blockSignals(False)
          
          
    @staticmethod
    def parse_prompt(template: str, instance: SelectionData, manager: SelectionsManager, discover_size: int) -> str:
        """Replace all `{{var}}` placeholders with instance values from `SelectionData`.
        
        Validates all placeholders exist as properties in `SelectionData` before replacement.
        Shows alert and returns None if any placeholder cannot be resolved.
        
        Args:
            template (str): Prompt template with `{{variable}}` placeholders.
            instance (SelectionData): Selection data instance to extract values from.
            
        Returns:
            str: Processed prompt with placeholders replaced, or None if validation fails.
        """
        
        valid = LlmConfigWidget.validate_prompt(template)
        if not valid:
            return None
        
        # If all placeholders are valid, perform the replacements
        def replacer(match):
            var_name = match.group(1)
            
            if var_name == LlmConfigWidget.PLACEHOLDER_TITLE_PATH:
                value = manager.get_selection_path_str(instance.id_, include_last=False)
            elif var_name == LlmConfigWidget.PLACEHOLDER_CONTEXT:
                value = manager.contextualize_selection(instance.id_,  discover_size) # LlmConfigWidget.PLACEHOLDER_CONTEXT_SIZE) 
            else:
                value = SelectionData.get_attr(instance, var_name)
            return str(value) if value is not None else ""

        return re.sub(r"\{\{(\w+)\}\}", replacer, template)
   

    @staticmethod
    def validate_prompt(template: str) -> bool:
        """Validate that all `{{var}}` placeholders in template are valid `SelectionData` properties.
        
        Extracts all placeholders and checks if they exist as properties in `SelectionData`.
        Shows alert for any invalid placeholders found.
        
        Args:
            template (str): Prompt template with `{{variable}}` placeholders to validate.
            
        Returns:
            bool: True if all placeholders are valid, False if any invalid placeholders found.
        """

        placeholders = re.findall(r"\{\{([^}]+)\}\}", template)
        # Validate all placeholders
        for var_name in placeholders:
            if var_name not in LlmConfigWidget.SPECIAL_PROMPT_PLACEHOLDER and not SelectionData.has_property(var_name):
                QMessageBox.warning(None, "Error", f"Cannot replace placeholder: `{{{var_name}}}`.")
                return False
        return True



class AugmentInteractiveDialog(QDialog):
    """Dialog for interactive LLM-based augmentation of selection descriptions.
    
    Provides interface for reviewing and editing selection data, configuring LLM parameters,
    and generating descriptions using AI. Allows users to accept, skip, or cancel the process.
    """

    def __init__(self, caller: QWidget, selection_data: SelectionData, selection_manager: SelectionsManager,llm_inputs: LlmInputs, title: str, parent: QWidget = None):
        """Initialize interactive augmentation dialog.
        
        Args:
            caller (QWidget): Parent widget that called this dialog.
            selection_data (SelectionData): Selection data to augment.
            llm_inputs (LlmInputs): LLM configuration parameters.
            title (str): Dialog window title.
            parent (QWidget, optional): Parent widget.
            
        Returns:
            None
        """
        
        super().__init__(parent=parent)
        self.setWindowTitle(title)
        
        self.selection_manager = selection_manager
        
        augment_layout = QVBoxLayout()
        augment_widget = QWidget()
        augment_widget.setLayout(augment_layout)
        self.config_widget = LlmConfigWidget(llm_inputs, allow_api_config=False)
        augment_layout.addWidget(self.config_widget)

        self.desc_edit = QTextEdit()
        self.desc_edit.setPlainText(selection_data.description)
        self.desc_label = QLabel("Description:")
        self.desc_edit.setPlaceholderText("Generated or edited description here")
        augment_layout.addWidget(self.desc_label)
        augment_layout.addWidget(self.desc_edit)

        btns = QHBoxLayout()        
        next_btn = QPushButton("Accept and Proceed")
        augment_btn = QPushButton("Describe with LLM") # TODO check the cancel button, it has a bug!?
        skip_btn = QPushButton("Skip")
        cancel_btn = QPushButton("Cancel")
        btns.addWidget(next_btn)
        btns.addWidget(augment_btn)
        btns.addWidget(skip_btn)
        btns.addWidget(cancel_btn)
        augment_layout.addLayout(btns)
        
        self.selection_viewer = SelectionDataEditingWidget(selection_data, parent=self, show_description=False)
        
        main_layout = QSplitter(Qt.Horizontal)
        main_layout.addWidget(augment_widget)
        main_layout.addWidget(self.selection_viewer)
        main_layout.setSizes([300, 700])  # Initial width proportions
        super_layout = QHBoxLayout(self)
        super_layout.addWidget(main_layout)
    
        self.setLayout(super_layout) #augment_layout)

        next_btn.clicked.connect(self.on_next)
        augment_btn.clicked.connect(self.on_augment)
        skip_btn.clicked.connect(self.on_skip)
        cancel_btn.clicked.connect(self.on_cancel)
        
        self.selection_data = selection_data
        self.caller = caller

        
    def on_next(self) -> None:
        """Accept current changes and proceed to next selection."""

        self.accept()


    def on_augment(self) -> None:
        """Trigger LLM augmentation process for current selection. The computation is based on `AugmentConfigDialog` and it is performed asynchronously.
        This method is triggered by clicking on the augment button, and it sets the `self.on_llm_finished`, `self.on_llm_error` and `self.on_llm_cancelled` callbacks."""

        # Invoke LLM
        msg = f"Please wait..."
        llm_inputs = self.config_widget.get_llm_inputs(self.selection_data, self.selection_manager)
        worker, dialog = AugmentConfigDialog.prepare_next_selection(
            llm_inputs, msg,
            finish_callback = lambda result : self.on_llm_finished(dialog, result),
            error_callback = lambda error: self.on_llm_error(dialog, error),
            stop_callback = lambda : self.on_llm_cancelled(dialog, worker)
        )
        worker.start()
        dialog.exec_()
        
        
    def on_llm_finished(self, dialog: LoadingDialog, result: LlmOutput) -> None:
        """Handle successful LLM response by updating description text editor.
        This function is invoked by `LlmWorker` through the `finished` signal which is triggered after than the LLM has been invoked (as configured in the `self.on_augment` function).
        
        Args:
            dialog (LoadingDialog): The loading dialog to close.
            result (LlmOutput): LLM response containing generated description.
            
        Returns:
            None
        """    
     
        dialog.accept()
        self.desc_label.setText(f"Description (in token: {result.in_token}, out token: {result.out_token}, time: {result.time} sec):")
        print(f"Got LLM response: {result}\n\t\t\t\t########")
        self.desc_edit.setPlainText(result.response)


    def on_llm_error(self, dialog: LoadingDialog, error: Exception) -> None:
        """Handle LLM error by showing alert and closing dialog.
        This function is invoked by `LlmWorker` through the `error` signal which is triggered in case of exceptions while invoking the LLM (as configured in the `self.on_augment` function).
        
        Args:
            dialog (LoadingDialog): The loading dialog to close.
            error (Exception): The error that occurred during LLM processing.
            
        Returns:
            None
        """

        dialog.reject()
        self.reject()
        QMessageBox.warning(self, "Error", f"Error while invoking LLM interactively.\n{error}")


    def on_llm_cancelled(self, dialog: LoadingDialog, worker: LlmWorker) -> None:
        """Handle LLM cancellation by stopping worker and closing dialogs.
        This function is invoked by `LoadingDialog` through the `stop_requested` signal which is triggered in case the user presses the cancel button (as configured in the `self.on_augment` function).
        
        Args:
            dialog (LoadingDialog): The loading dialog to close.
            worker (LlmWorker): The LLM worker thread to cancel.
            
        Returns:
            None
        """
    
        worker.cancel()
        dialog.reject()
        self.reject()
        self.loop_cancelled = True


    def get_section_description(self) -> SelectionData:
        """Get updated selection data with current description from text editor.
        It is used by `AugmentConfigDialog`, when the processing button is pressed. to set up data.
        
        Returns:
            SelectionData: Updated selection data with new description.
        """
    
        data = self.selection_viewer.get_data()
        data.description = self.desc_edit.toPlainText()
        return data

        
    def on_skip(self) -> None:
        """Skip current selection without changes. This is triggered by clicking on the skip button."""

        self.reject()


    def on_cancel(self) -> None:
        """Cancel entire augmentation process. This is triggered by clicking on the cancel button."""
        
        self.caller.loop_cancelled = True
        self.reject()



# The dialog to set up the augmentation through LLM
class AugmentConfigDialog(QDialog):
    """
    Dialog for configuring and executing augmentation of PDF selections using an LLM.
    This dialog allows users to:
    - Select augmentation options such as interactive mode and skipping already described selections.
    - Specify page ranges to process.
    - Choose categories and configure LLM inputs via a configuration widget.
    - Review and edit augmented descriptions interactively or process them in batch mode.
    - Handles LLM invocation, error reporting, and updating selection descriptions.
    Args:
        main_view: Reference to the main view/controller, used for accessing selections and updating data.
    Attributes:
        cancelled (bool): Indicates if the dialog was cancelled.
        current_selection: The currently processed selection.
        config_widget: Widget for configuring LLM options and categories.
        interactive_cb: Checkbox for enabling interactive mode.
        skip_described_cb: Checkbox for skipping selections that already have descriptions.
        page_edit: Line edit for specifying page ranges.
        loop_cancelled (bool): Indicates if the augmentation loop was cancelled.
    Methods:
        on_proceed(): Handles the logic for processing selections based on user configuration.
        prepare_next_selection(): Static method to prepare LLM worker and loading dialog.
        on_llm_cancelled(): Handles cancellation of LLM processing.
        on_llm_finished(): Handles successful completion of LLM processing and updates selection.
        set_selection_description(): Updates the description of the current selection.
        on_llm_error(): Handles errors during LLM invocation.
    """
    
    def __init__(self, main_view: 'PDFAnnotationTool'):
        """
        Initialize the augmenting widget.
        Args:
            main_view: The main view object to interact with.
        Sets up the UI components including configuration widget, interactive mode checkbox,
        skip described checkbox, page selection input, and Cancel/Proceed buttons.
        Connects button signals to their respective handlers.
        """
    
        super().__init__()
        self.main_view = main_view
        self.cancelled = False
        self.current_selection = None
        
        layout = QVBoxLayout(self)
        self.config_widget = LlmConfigWidget()
        layout.addWidget(self.config_widget)

        self.interactive_cb = QCheckBox("Interactive mode")
        self.interactive_cb.setChecked(True)
        layout.addWidget(self.interactive_cb)

        self.skip_described_cb = QCheckBox("Skip already described")
        layout.addWidget(self.skip_described_cb)

        self.page_edit = QLineEdit()
        layout.addWidget(QLabel("Pages, e.g., 2,3,5-7 (leave empty for all pages):"))
        layout.addWidget(self.page_edit)

        btns = QHBoxLayout()
        cancel_btn = QPushButton("Cancel")
        proceed_btn = QPushButton("Proceed")
        btns.addWidget(cancel_btn); btns.addWidget(proceed_btn)
        layout.addLayout(btns)

        self.setLayout(layout)

        cancel_btn.clicked.connect(self.close)
        proceed_btn.clicked.connect(self.on_proceed)
    
    
    def on_proceed(self) -> None:
        """
        Handles the process of augmenting selected sections based on user input.
        This method performs the following steps:
        1. Retrieves the page range specified by the user and validates it.
        2. Filters selections based on allowed pages and selected categories.
        3. Optionally skips selections that already have a description.
        4. If no matching selections are found, notifies the user.
        5. Iterates through filtered selections and invokes the LLM for augmentation:
            - In interactive mode, displays a dialog for user review and updates the description.
            - In non-interactive mode, processes selections asynchronously and updates the description.
        6. Notifies the user when all selections have been processed.
        Returns:
            None
        """
        
        # Get optional user input: PAGE RANGE
        page_range = self.page_edit.text().strip()
        allowed_pages = self.main_view.parse_page_range(page_range) # It shows alert in case of error
        if allowed_pages is None: 
            return

        # Retrieve the selection to process based on user input
        allowed_categories = self.config_widget.get_selected_categories()
        if allowed_categories is None:
                return
        filtered_sections = []
        for page_number, selection_list in self.main_view._selections.items():
            if page_number not in allowed_pages: 
                continue
            
            for sec in selection_list:
                if sec.data.category.value.name  in allowed_categories:
                    if self.skip_described_cb.isChecked():
                        if sec.data.description.strip() == "":
                            filtered_sections.append(sec)
                    else:
                        filtered_sections.append(sec)
        if not filtered_sections:
            QMessageBox.information(self, "No match", "No selections found.")
            return
        
        selection_manager = self.main_view._selections

        # Loop and invoke LLM for each selection
        cnt = 1
        self.loop_cancelled = False
        for s in filtered_sections:
            self.current_selection = s                        
            llm_inputs = self.config_widget.get_llm_inputs(s.data, selection_manager)
            if llm_inputs is None:
                self.loop_cancelled = True
            if self.loop_cancelled:
                break
            #print(f"\tAugmenting with {llm_inputs}")
            
            if self.interactive_cb.isChecked():
                title = f"Review Selection {cnt} of {len(filtered_sections)}"
                dialog = AugmentInteractiveDialog(self, s.data, selection_manager, llm_inputs, title)
                accepted = dialog.exec_()
                self.set_selection_description(dialog.get_section_description())
                if accepted:
                    self.main_view.autosave_json()
            else:    
                # Invoke LLM
                msg = f"Please wait... Selection {cnt} of {len(filtered_sections)}."
                worker, dialog = self.prepare_next_selection(
                    llm_inputs, msg,
                    finish_callback = lambda result : self.on_llm_finished(dialog, result),
                    error_callback = lambda error: self.on_llm_error(dialog, error),
                    stop_callback = lambda : self.on_llm_cancelled(dialog, worker)
                )
                worker.start()
                dialog.exec_()
                
            cnt += 1
        
        if cnt >= len(filtered_sections) and len(filtered_sections) > 0:
            QMessageBox.information(self, "Done", "All selections processed!") # TODO add statistics
        

    @staticmethod
    def prepare_next_selection(llm_inputs: LlmInputs, message: str, finish_callback: callable, error_callback: callable, stop_callback: callable) -> Tuple[LlmWorker, LoadingDialog]:
        """Prepare LLM worker and loading dialog for processing a selection.
        
        Args:
            llm_inputs (LlmInputs): LLM configuration for the request.
            message (str): Message to display in loading dialog.
            finish_callback (callable): Callback for successful completion.
            error_callback (callable): Callback for error handling.
            stop_callback (callable): Callback for user cancellation.
            
        Returns:
            Tuple[LlmWorker, LoadingDialog]: Worker thread and loading dialog instances.
        """
    
        worker = LlmWorker(llm_inputs)
        dialog = LoadingDialog(message)
        
        worker.finished.connect(finish_callback)
        worker.error.connect(error_callback)
        dialog.stop_requested.connect(stop_callback)
        
        return worker, dialog


    def on_llm_cancelled(self, dialog: LoadingDialog, worker: LlmWorker) -> None:
        """Handle LLM cancellation in batch mode by stopping worker and setting cancel flag.
        
        Args:
            dialog (LoadingDialog): The loading dialog to close.
            worker (LlmWorker): The LLM worker thread to cancel.
            
        Returns:
            None
        """
        
        worker.cancel()
        dialog.reject()
        self.loop_cancelled = True
    
    
    def on_llm_finished(self, dialog: LoadingDialog, result: LlmOutput) -> None:
        """Handle successful LLM response in batch mode by updating selection description.
        
        Args:
            dialog (LoadingDialog): The loading dialog to close.
            result (LlmOutput): LLM response containing generated description.
            
        Returns:
            None
        """
    
        print(f"\tGot LLM response: {result}\n\t\t\t\t********")
        if self.current_selection:
            editing_key = self.current_selection.data.page
            editing_idx = self.current_selection.data.idx
            new_selection = self.current_selection.copy()
            new_selection.data.description = result.response
            self.main_view.edit_selection(editing_key, editing_idx, new_selection) 
        dialog.accept()
        self.main_view.autosave_json()

              
    def set_selection_description(self, augmented_data: SelectionData) -> None:
        """Update the current selection with new description data.
        
        Args:
            selection_data (SelectionData): Updated selection data with new description.
            
        Returns:
            None
        """
    
        if self.current_selection:
            editing_key = self.current_selection.data.page
            editing_idx = self.current_selection.data.idx
            new_selection = self.current_selection.copy()
            new_selection.data = augmented_data
            self.main_view.edit_selection(editing_key, editing_idx, new_selection) 
        
        
    def on_llm_error(self, dialog: LoadingDialog, error: Exception) -> None:
        """Handle LLM error in batch mode by showing alert and setting cancel flag.
        
        Args:
            dialog (LoadingDialog): The loading dialog to close.
            error (Exception): The error that occurred during LLM processing.
            
        Returns:
            None
        """

        dialog.reject()
        QMessageBox.warning(self, "Error", f"Error while invoking LLM.\n{error}")





